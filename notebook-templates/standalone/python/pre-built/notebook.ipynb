{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Error Analysis - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the [Model Error Analysis plugin](https://www.dataiku.com/product/plugins/model-error-analysis/) and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiku.use_plugin_libs('model-error-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dku_error_analysis_mpp.dku_error_visualizer import DkuErrorVisualizer\n",
    "from dku_error_analysis_mpp.dku_error_analyzer import DkuErrorAnalyzer\n",
    "from dku_error_analysis_model_parser.model_handler_utils import get_model_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained primary model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load any trained DSS classification or regression model. This is your _Primary Model_.\n",
    "\n",
    "Replace the `lookup` and `version_id` placeholders with your own model identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup='5V4iiYuK'\n",
    "version_id = None#'initial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an accessor to your model through `get_model_handler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dataiku.Model(lookup)\n",
    "model_handler = get_model_handler(m, version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a DkuErrorAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `DkuErrorAnalyzer` object with your model accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_analyzer = DkuErrorAnalyzer(model_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the underlying <font color=red>_Error Tree_ </font> to your DSS model performances on its test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_analyzer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying Error Tree can be retrieved by the attribute `_error_tree`.\n",
    "\n",
    "You can see that its estimator consists in a `DecisionTree` from `sklearn` predicting _Correct_ or _Wrong Prediction_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_clf = dku_error_analyzer._error_tree._estimator\n",
    "print(error_clf)\n",
    "print(error_clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features used in the Error Tree can be retrieved by the attribute `preprocessed_feature_names`. \n",
    "\n",
    "These features are the very same used by your primary model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dku_error_analyzer.preprocessed_feature_names\n",
    "feature_names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Tree Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a report on the Error Tree thanks to the `evaluate` function as a text or formatted output. \n",
    "\n",
    "This will output some metrics computed on a part of the test set of your DSS model:\n",
    "1. the Error Tree accuracy score\n",
    "2. the estimated accuracy of your primary model according to the Error Tree\n",
    "3. the true accuracy of your primary model\n",
    "4. the _Fidelity_ of the Error Tree (absolute deviation of 2. and 3.)\n",
    "\n",
    "Ideally the two values 2. and 3. above should be equal, thus their deviation (_Fidelity_) is computed as an indicator of how well the Error Tree represents your model performances.\n",
    "\n",
    "The _Confidence Decision_ states whether you can trust the Error Tree as a representation of your model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dku_error_analyzer.evaluate(output_format='str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_analyzer.evaluate(output_format='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a DkuErrorVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `DkuErrorVisualizer` object on your `DkuErrorAnalyzer` object in order to have useful plot and analysis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_visualizer = DkuErrorVisualizer(dku_error_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Error Tree decision tree and have a look at the red nodes, representing your primary model failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dku_error_visualizer.plot_error_tree(size=(25, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Error Tree nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `get_error_leaf_summary` function of the `DkuErrorAnalyzer` to explore the nodes and have information about the samples they contain. \n",
    "\n",
    "The provided information covers:\n",
    "1. the number of correct predictions,\n",
    "2. the number of wrong predictions,\n",
    "3. the node _Local error_: the ratio of the number of wrongly predicted samples over the total number of samples in the node. This is the error rate in the node. It is interesting to focus on nodes where the local error rate is higher than the average error rate of the model on all samples. \n",
    "4. the node _Global error_: the ratio of the number of wrongly predicted samples over the total number of mispredicted samples in the test set. The nodes where the global erro is high is where the majority of wrong predictions are located.\n",
    "5. the path to node: showing roughly the features behaviour for the samples in the node. Helps understanding what feature ranges are contributing the most to the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different nodes contain meaningful segments of the test set, and represent different types of errors the primary model makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are especially interested in nodes with high Global Error (majority of errors) and high Local Error (the error rate in the subgroup of samples in the node). Especially if the local error is much higher than the average error rate of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can input leaf nodes from the tree plot above (`leaf_selector` argument).\n",
    "\n",
    "Replace `leaf_id` with the leaf node you would like to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_analyzer.get_error_leaf_summary(leaf_selector=leaf_id, add_path_to_leaves=True, output_format='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also let the analyzer show you all the leaf nodes ranked by importance (higher global error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dku_error_analyzer.get_error_leaf_summary(add_path_to_leaves=True, output_format='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Feature Distributions of samples in the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `DkuErrorVisualizer` to plot the histograms of the features in the nodes, comparing to the global population as it is a mainly successful baseline.\n",
    "\n",
    "Again, you can either input leaf nodes by the `leaf_selector` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dku_error_visualizer.plot_feature_distributions_on_leaves(leaf_selector=leaf_id, top_k_features=1, show_class=True, show_global=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also let the visualizer show you the feature distributions in all the leaf nodes ranked by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dku_error_visualizer.plot_feature_distributions_on_leaves(top_k_features=1, show_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we observe that the primary model yields wrong predictions for houses with very large living rooms and high scored views. Comparing with the global baseline, we see that these samples are under-represented in the primary training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model Error Analysis plugin automatically highlights any information relevant to the model errors, leading the user to focus on what are the problematic features and what are the typical values of these features for the mispredicted samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information can later be exploited to support the strategy selected by the user :\n",
    "* **improve model design**: removing a problematic feature, removing samples likely to be mislabeled, ensemble with a model trained on a problematic subpopulation, ...\n",
    "* **enhance data collection**: gather more data regarding the most erroneous under-represented populations,\n",
    "* **select critical samples for manual inspection** thanks to the Error Tree and avoid primary predictions on them, generating model assertions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "customFields": {},
  "kernelspec": {
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
